{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities import *\n",
    "from Data.Boussineq_2D_Data import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from functorch import jacrev,vmap,vjp\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "'''\n",
    "Script to recreate HFM\n",
    "'''\n",
    "\n",
    "def aux_wrap(net):\n",
    "    def aux_net(xyt):\n",
    "        result = net(xyt)\n",
    "        return result,result\n",
    "    return aux_net\n",
    "def get_conc_derivs(net,xyt,hes_vecs):\n",
    "    '''\n",
    "    Get the all the derivatives for the equations for Navier Stokes and Concentration Equations\n",
    "    Assumes the NN Model maps: (x,y,t) --> (u,v,p,c)\n",
    "\n",
    "    Uses Functorch a JAX like interface to get derivatives\n",
    "    '''\n",
    "    aux_net = aux_wrap(net)\n",
    "    #Crazy One Functional liner gives returns (Jacobian,Hessian function, Network Evaluation) for input xyt = (x,y,t)\n",
    "    jac,partial_hess,out = vjp(jacrev(aux_net,has_aux = True),xyt,has_aux = True)\n",
    "\n",
    "    u,v,p,temp = out\n",
    "\n",
    "    #Jacobian:\n",
    "    ## x        y       t\n",
    "    #u ux       uy      ut\n",
    "    #v vx       vy      vt\n",
    "    #p px       py      pt\n",
    "    #c temp_x  temp_y   temp_t\n",
    "    ux,uy,ut = jac[0,:]\n",
    "    vx,vy,vt = jac[1,:]\n",
    "    px,py    = jac[2,0:2]\n",
    "    temp_x,temp_y,temp_t = jac[3,:]\n",
    "    \n",
    "    #The partial hess allows us to get the derivatives of specific elements in the Jacobian\n",
    "    #Much more effecient than calulating entire hessian\n",
    "    d2 = vmap(partial_hess)(hes_vecs)[0]\n",
    "\n",
    "    #Second Order Terms\n",
    "    uxx,uyy = d2[[0,1],[0,1]]\n",
    "    vxx,vyy = d2[[2,3],[0,1]]\n",
    "    temp_xx,temp_yy = d2[[4,5],[0,1]]\n",
    "\n",
    "    return (p,px,py), (temp,temp_x,temp_y,temp_t,temp_xx,temp_yy),(u,ux,uy,ut,uxx,uyy),(v,vx,vy,vt,vxx,vyy) \n",
    "    \n",
    "# Code for one epoch\n",
    "def one_epoch(train_loader,net,optimizer,Re = 100, bouy = 100,alpha = 100, device = 'cpu',weights = None):\n",
    "    vecs = torch.zeros((6,4,3))\n",
    "    #These tensors allow us to effeciently get 2nd order derivatives without\n",
    "    #Computing the entire Hessian\n",
    "    #uxx uyy\n",
    "    vecs[0,0,0] = 1\n",
    "    vecs[1,0,1] = 1\n",
    "\n",
    "    #vxx vyy\n",
    "    vecs[2,1,0] = 1\n",
    "    vecs[3,1,1] = 1\n",
    "\n",
    "    #Cxx, Cyy\n",
    "    vecs[4,3,0] = 1\n",
    "    vecs[5,3,1] = 1\n",
    "    vecs = vecs.to(device)\n",
    "\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [1.]*6\n",
    "    #Variables to keep track of total loss and individual losses (unweighted)\n",
    "    running_loss = 0.\n",
    "    indi_loss = torch.zeros(6)\n",
    "\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        xyt,u_data,v_data = data[0].to(device),data[1].to(device),data[2].to(device)\n",
    "        (p,px,py), (temp,temp_x,temp_y,temp_t,temp_xx,temp_yy),(u,ux,uy,ut,uxx,uyy),(v,vx,vy,vt,vxx,vyy) = vmap(get_conc_derivs,(None,0,None))(net,xyt,vecs)\n",
    "        #we output u,y,T-T0 as T-T0 scales to -1 and 1. We can recover original temp by adding T0\n",
    "\n",
    "        #Boussineq stokes eq\n",
    "        e1 = ut + u*ux+v*uy + px - 1/Re*(uxx + uyy) \n",
    "        #y direction has additional buoyancy term\n",
    "        e2 = vt + vx*u +vy*v + py - 1/Re*(vxx+vyy) + bouy*(temp)\n",
    "        #Incompress\n",
    "        e3 = ux+vy\n",
    "        #Concentration eq\n",
    "        e4 = temp_t + (u*temp_x + v*temp_y) - alpha*(temp_xx+temp_yy)\n",
    "        # Data Fitting\n",
    "        e5 = u-u_data \n",
    "        e6 = v-v_data\n",
    "        eq_s = [e1,e2,e3,e4,e5,e6]\n",
    "\n",
    "        #Sum of MSE of each loss\n",
    "        \n",
    "        total_loss = sum([w*e.pow(2).mean() for w,e in zip(weights,eq_s)])\n",
    "        \n",
    "        #Get Gradients then update weights\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Loss Tracking\n",
    "        running_loss += total_loss\n",
    "        indi_loss += tensor([e.pow(2).mean() for e in eq_s])\n",
    "        \n",
    "    return running_loss ,indi_loss \n",
    "\n",
    "class Boussineq_Dataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        data = self.data[idx,:]\n",
    "        xyt = data[0:3]\n",
    "        u = data[3]\n",
    "        v = data[4]\n",
    "        return xyt,u,v\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Starting Run at time 2023_01_19_17_25_07 using device cpu \n",
      "\n",
      "\n",
      "Dimensional Numbers: Re 104.067, Buoy -2236.680, alpha 0.0001452\n",
      "Time (H) 0.001 Epoch 0 loss 85790.945312 Equation Losses \t tensor([1.0914e-07, 8.5790e+04, 1.7679e-06, 1.1904e-06, 4.9092e-02, 4.1816e-01])\n",
      "Saved!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\johnc\\Documents\\Sydney Uni\\2022\\Summer Research\\Summer Research\\Boussineq_2D.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m timer_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MAX_EPOCHS):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39m#One epoch training\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     loss,indi_loss \u001b[39m=\u001b[39m one_epoch(train_loader,net,optimizer,Re \u001b[39m=\u001b[39;49m Re, bouy \u001b[39m=\u001b[39;49m bouy, alpha \u001b[39m=\u001b[39;49m alpha, device \u001b[39m=\u001b[39;49m device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39m#Record Losses and elapsed time\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     epoch_losses\u001b[39m.\u001b[39mappend( [i,\u001b[39mfloat\u001b[39m(loss)] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mfloat\u001b[39m,indi_loss)) )\n",
      "\u001b[1;32mc:\\Users\\johnc\\Documents\\Sydney Uni\\2022\\Summer Research\\Summer Research\\Boussineq_2D.ipynb Cell 2\u001b[0m in \u001b[0;36mone_epoch\u001b[1;34m(train_loader, net, optimizer, Re, bouy, alpha, device, weights)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m xyt,u_data,v_data \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device),data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device),data[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m (p,px,py), (temp,temp_x,temp_y,temp_t,temp_xx,temp_yy),(u,ux,uy,ut,uxx,uyy),(v,vx,vy,vt,vxx,vyy) \u001b[39m=\u001b[39m vmap(get_conc_derivs,(\u001b[39mNone\u001b[39;49;00m,\u001b[39m0\u001b[39;49m,\u001b[39mNone\u001b[39;49;00m))(net,xyt,vecs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m#we output u,y,T-T0 as T-T0 scales to -1 and 1. We can recover original temp by adding T0\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m#Boussineq stokes eq\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m e1 \u001b[39m=\u001b[39m ut \u001b[39m+\u001b[39m u\u001b[39m*\u001b[39mux\u001b[39m+\u001b[39mv\u001b[39m*\u001b[39muy \u001b[39m+\u001b[39m px \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39mRe\u001b[39m*\u001b[39m(uxx \u001b[39m+\u001b[39m uyy) \n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\vmap.py:362\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m _check_out_dims_is_int_or_int_pytree(out_dims, func)\n\u001b[0;32m    361\u001b[0m batch_size, flat_in_dims, flat_args, args_spec \u001b[39m=\u001b[39m _process_batched_inputs(in_dims, args, func)\n\u001b[1;32m--> 362\u001b[0m \u001b[39mreturn\u001b[39;00m _flat_vmap(\n\u001b[0;32m    363\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    364\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\vmap.py:35\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[1;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\vmap.py:489\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[1;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     batched_inputs \u001b[39m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[1;32m--> 489\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39mbatched_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[0;32m    491\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Users\\johnc\\Documents\\Sydney Uni\\2022\\Summer Research\\Summer Research\\Boussineq_2D.ipynb Cell 2\u001b[0m in \u001b[0;36mget_conc_derivs\u001b[1;34m(net, xyt, hes_vecs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m aux_net \u001b[39m=\u001b[39m aux_wrap(net)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#Crazy One Functional liner gives returns (Jacobian,Hessian function, Network Evaluation) for input xyt = (x,y,t)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m jac,partial_hess,out \u001b[39m=\u001b[39m vjp(jacrev(aux_net,has_aux \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m),xyt,has_aux \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m u,v,p,temp \u001b[39m=\u001b[39m out\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m#Jacobian:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m## x        y       t\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m#u ux       uy      ut\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m#v vx       vy      vt\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m#p px       py      pt\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnc/Documents/Sydney%20Uni/2022/Summer%20Research/Summer%20Research/Boussineq_2D.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m#c temp_x  temp_y   temp_t\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\eager_transforms.py:262\u001b[0m, in \u001b[0;36mvjp\u001b[1;34m(func, has_aux, *primals)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvjp\u001b[39m(func: Callable, \u001b[39m*\u001b[39mprimals, has_aux: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    168\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m    Standing for the vector-Jacobian product, returns a tuple containing the\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m    results of :attr:`func` applied to :attr:`primals` and a function that, when\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39m        should not depend on the result of a context manager outside of ``f``.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     \u001b[39mreturn\u001b[39;00m _vjp_with_argnums(func, \u001b[39m*\u001b[39;49mprimals, has_aux\u001b[39m=\u001b[39;49mhas_aux)\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\vmap.py:35\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[1;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\eager_transforms.py:289\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[1;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[0;32m    287\u001b[0m     diff_primals \u001b[39m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    288\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[39m=\u001b[39mlevel), diff_primals)\n\u001b[1;32m--> 289\u001b[0m primals_out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mprimals)\n\u001b[0;32m    291\u001b[0m \u001b[39mif\u001b[39;00m has_aux:\n\u001b[0;32m    292\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(primals_out, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(primals_out) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\eager_transforms.py:474\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    471\u001b[0m flat_basis \u001b[39m=\u001b[39m _construct_standard_basis_for(flat_output, flat_output_numels)\n\u001b[0;32m    472\u001b[0m basis \u001b[39m=\u001b[39m tree_unflatten(flat_basis, output_spec)\n\u001b[1;32m--> 474\u001b[0m results \u001b[39m=\u001b[39m vmap(vjp_fn)(basis)\n\u001b[0;32m    476\u001b[0m primals \u001b[39m=\u001b[39m _slice_argnums(args, argnums)\n\u001b[0;32m    477\u001b[0m flat_primals, primals_spec \u001b[39m=\u001b[39m tree_flatten(primals)\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\vmap.py:362\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m _check_out_dims_is_int_or_int_pytree(out_dims, func)\n\u001b[0;32m    361\u001b[0m batch_size, flat_in_dims, flat_args, args_spec \u001b[39m=\u001b[39m _process_batched_inputs(in_dims, args, func)\n\u001b[1;32m--> 362\u001b[0m \u001b[39mreturn\u001b[39;00m _flat_vmap(\n\u001b[0;32m    363\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    364\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\vmap.py:35\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[1;32m---> 35\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\vmap.py:489\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[1;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     batched_inputs \u001b[39m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[1;32m--> 489\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39mbatched_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[0;32m    491\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\eager_transforms.py:323\u001b[0m, in \u001b[0;36m_vjp_with_argnums.<locals>.wrapper\u001b[1;34m(cotangents, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mif\u001b[39;00m primals_out_spec \u001b[39m!=\u001b[39m cotangents_spec:\n\u001b[0;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mExpected pytree structure of cotangents to be the same \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mas pytree structure of outputs to the function. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcotangents: \u001b[39m\u001b[39m{\u001b[39;00mtreespec_pprint(cotangents_spec)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    322\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprimal output: \u001b[39m\u001b[39m{\u001b[39;00mtreespec_pprint(primals_out_spec)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 323\u001b[0m result \u001b[39m=\u001b[39m _autograd_grad(flat_primals_out, flat_diff_primals, flat_cotangents,\n\u001b[0;32m    324\u001b[0m                         retain_graph\u001b[39m=\u001b[39;49mretain_graph, create_graph\u001b[39m=\u001b[39;49mcreate_graph)\n\u001b[0;32m    325\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(result, primals_spec)\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\functorch\\_src\\eager_transforms.py:113\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(diff_outputs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mzeros_like(inp) \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m inputs)\n\u001b[1;32m--> 113\u001b[0m grad_inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(diff_outputs, inputs, grad_outputs,\n\u001b[0;32m    114\u001b[0m                                   retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[0;32m    115\u001b[0m                                   create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[0;32m    116\u001b[0m                                   allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    117\u001b[0m grad_inputs \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mzeros_like(inp) \u001b[39mif\u001b[39;00m gi \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m gi\n\u001b[0;32m    118\u001b[0m                     \u001b[39mfor\u001b[39;00m gi, inp \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(grad_inputs, inputs))\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m grad_inputs\n",
      "File \u001b[1;32mc:\\Users\\johnc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    301\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[0;32m    302\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    SAVE_PATH = 'Save/Boussineq'\n",
    "    MAX_HOUR_DURATION = 40\n",
    "    MAX_EPOCHS = 10\n",
    "\n",
    "    if not os.path.exists(SAVE_PATH):\n",
    "        os.mkdir(SAVE_PATH)\n",
    "\n",
    "    #Set Network Up\n",
    "    net = HFM_Net(3,4,10,200)\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    # net.non_lin = torch.tanh\n",
    "    net = net.to(device)\n",
    "\n",
    "\n",
    "    #Data Wrangling see HFM_training data for how data works (from HFM paper)\n",
    "    x_data,y_data,t_data,u_data,v_data,temp_data,p_data = get_boussineq_2D('Data/DHC_2D_Unsteady.dat')\n",
    "\n",
    "    #Dimensional Scales\n",
    "    RHO = 998\n",
    "    L = 1\n",
    "    U = 1e-3\n",
    "    MU = 0.00959\n",
    "    #For Temp inthis case leave as 1 \n",
    "    T = 1 \n",
    "    #Thermal Expansion\n",
    "    BETA = 0.000228\n",
    "    #Conductivity #specific Heat\n",
    "    k,Cp = 0.606,4181\n",
    "    alpha_star = k/(Cp*RHO)\n",
    "\n",
    "    Re = RHO*U*L/MU\n",
    "    bouy = -9.81*L*BETA/(U**2)\n",
    "    alpha = alpha_star/(U*L)\n",
    "\n",
    "    # Rescale All values\n",
    "    x_data = x_data/L\n",
    "    y_data = y_data/L\n",
    "    t_data = t_data/(L/U)\n",
    "\n",
    "    u_data = u_data/U\n",
    "    v_data = v_data/U\n",
    "    p_data = p_data/(RHO*U**2)\n",
    "    temp_data = temp_data/T\n",
    "\n",
    "\n",
    "    data = np.stack([x_data,y_data,t_data,u_data,v_data], axis = 1)\n",
    "    new_data = tensor(data,dtype = torch.float32).squeeze()\n",
    "    #DataLoader for Easy shuffling of data. Only Use 2_000_000 points\n",
    "    train_set = Boussineq_Dataset(new_data[0:2000,:])\n",
    "    train_loader = DataLoader(train_set,batch_size = 1000,shuffle = True,num_workers =0)\n",
    "    # a = iter(train_loader)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr = 1e-3)\n",
    "\n",
    "    #Tracking Loss\n",
    "    best_loss = float('inf')\n",
    "    currentDateAndTime = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    epoch_losses = []\n",
    "\n",
    "\n",
    "    print(f'\\n\\n Starting Run at time {currentDateAndTime} using device {device} \\n\\n')\n",
    "    print(f'Dimensional Numbers: Re {Re:.3f}, Buoy {bouy:.3f}, alpha {alpha:.7f}')\n",
    "\n",
    "\n",
    "    with open(f'{SAVE_PATH}/Run_{currentDateAndTime}.csv','w',newline = '') as f:\n",
    "\n",
    "        #Text file to track losses\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(['Epoch','Total Loss','NS_u','NS_y','Incompressibility','Conc_eq','Conc_data'])\n",
    "\n",
    "        timer_start = time.perf_counter()\n",
    "        for i in range(MAX_EPOCHS):\n",
    "            #One epoch training\n",
    "            loss,indi_loss = one_epoch(train_loader,net,optimizer,Re = Re, bouy = bouy, alpha = alpha, device = device)\n",
    "            #Record Losses and elapsed time\n",
    "            epoch_losses.append( [i,float(loss)] + list(map(float,indi_loss)) )\n",
    "            csv_writer.writerow(epoch_losses[-1])\n",
    "            elapsed_time = (time.perf_counter() - timer_start)/3600 \n",
    "            \n",
    "            if i % 2  == 0:\n",
    "                print(f'Time (H) {elapsed_time:.3f} Epoch {i} loss {loss:3f} Equation Losses \\t {indi_loss}') \n",
    "\n",
    "                if loss < best_loss:\n",
    "                    torch.save(net.to('cpu').state_dict(),f'{SAVE_PATH}/Best_Loss_run_{currentDateAndTime}.pth')\n",
    "                    best_loss = loss\n",
    "                    net.to(device)\n",
    "                    print('Saved!')\n",
    "            if elapsed_time > MAX_HOUR_DURATION:\n",
    "                break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93c0104d3f227b0e033a0cd6005bb89e2b2cf93a22dc7d60b19b22d2cfc418a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
